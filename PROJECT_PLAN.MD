# 유튜브 스트리머 텐션/재미도 예측 프로젝트 계획

## 프로젝트 개요

- **목적**: 동영상 클립에서 "재밌는 순간"을 자동으로 감지하고 타임스탬프 생성
- **접근법**: 감정 인식 모델 + 오디오 특징 + 시간축 패턴 학습을 통한 텐션 변화 분석
- **가정**: 재밌는 순간 = 텐션(감정) 변화 + 음성 변화와 비례

## 기술 스택 및 모델 구조

### 1. 사전 훈련된 감정 인식 모델

- **모델**: EfficientNet-B0 기반 VA-MTL (AffectNet 훈련)
- **파일**: `enet_b0_8_va_mtl.pt`
- **구조**:
    - 백본: EfficientNet-B0 (1280차원 특징 추출)
    - 분류기: 10차원 출력 (8개 감정 + VA 2차원)
    - 총 파라미터: 4,062,423개

### 2. 오디오 특징 추출

- **특징**: 프레임별 2차원 오디오 정보
    - RMS 에너지 (음량 크기)
    - 음량 변화율 (이전 프레임 대비)
- **라이브러리**: librosa, opencv
- **동기화**: 비디오 프레임(2fps)과 정확히 매칭

### 3. 시간축 패턴 학습 모델

- **입력**: 12차원 × 시간축 (감정 10차원 + 오디오 2차원)
- **모델 후보**:
    - LSTM: 감정+음성의 순차적 변화 패턴 학습
    - Conv1D: 멀티모달 패턴의 local feature 추출
- **출력**: 이진 분류 (재밌음/보통)

## 데이터 파이프라인

### 1. 영상 전처리

- **클립 길이**: 5~7초 (재밌는 순간을 담기에 적절)
- **프레임 추출**: 2fps (5초 = 10프레임, 7초 = 14프레임)
- **이미지 전처리**:
    - 얼굴 감지 및 crop
    - 224×224 리사이즈
    - ImageNet 정규화

### 2. 오디오 전처리

- **추출**: 비디오에서 오디오 스트림 분리
- **샘플링**: 22050Hz 기준
- **프레임 동기화**: 0.5초(1프레임)마다 오디오 특징 추출
- **특징 계산**:
    - RMS 에너지로 음량 측정
    - 이전 프레임 대비 변화율 계산

### 3. 멀티모달 특징 추출

- **단계**: 영상 → [프레임 시퀀스 + 오디오 시퀀스] → [VA 모델 + 오디오 분석] → 12차원 시계열 데이터
- **출력 형태**: `[프레임수 × 12차원]` 배열
    
    ```python
    frame_t: [감정_10차원] + [음량, 음량변화_2차원]
    ```
    

### 4. 가변 길이 처리

- **방법**: 패딩 (Padding) 사용
- **최대 길이**: 14프레임 (7초 기준)
- **구현**: 짧은 클립은 뒤에 0값 패딩, LSTM 마스킹 적용

## 데이터셋 구성

### 1. 데이터 수집

- **소스**: 침착맨 유튜브 영상
- **클립 기준**:
    - 재밌는 클립: 웃음소리, 댓글 반응, 조회수 급증 구간, 음량 급변 구간
    - 보통 클립: 일반 대화, 설명 구간, 일정한 음량 구간
- **라벨링**: 이진 분류 (재밌음=1, 보통=0)

### 2. 데이터 구조

```
클립 1: [12차원 × 10프레임] + [4개 제로 패딩] → 라벨: 1
클립 2: [12차원 × 14프레임] + [패딩 없음] → 라벨: 0

각 프레임 구성:
[감정1, 감정2, ..., 감정8, V값, A값, 음량, 음량변화]
```

## 구현 단계

### Phase 1: 모델 검증

1. ✅ **모델 구조 분석 완료** - 1280차원 특징, 10차원 분류 출력 확인
2. **VA 모델 테스트** - 10차원 출력의 정확한 구성 확인
3. **오디오 특징 추출 테스트** - librosa 기반 음량/변화율 계산 검증
4. **멀티모달 비디오 프로세서 구현** - 영상 → 12차원 시계열 변환

### Phase 2: 데이터 준비

5. **침착맨 클립 수집** - 재밌는/보통 클립 각 50-100개
6. **멀티모달 전처리 파이프라인** - 자동화된 영상+오디오 → 특징 추출
7. **라벨링 및 데이터셋 구성**

### Phase 3: 모델 학습

8. **Conv1D or LSTM 모델 구현** - 12차원 시퀀스 투 원 분류
9. **학습 및 검증** - 교차 검증으로 성능 평가
10. **하이퍼파라미터 튜닝**

### Phase 4: 응용

11. **실시간 처리** - 긴 영상에서 재밌는 구간 자동 탐지
12. **타임스탬프 생성** - 하이라이트 순간 자동 추출

## 핵심 아이디어

- **멀티모달 패턴**: "평온한 표정+조용함 → 놀란 표정+음량증가 → 웃는 표정+웃음소리" 같은 복합 시퀀스가 재미의 핵심
- **차원 효율성**: 1280차원 → 12차원으로 크게 축소하여 학습 안정성 확보
- **시공간적 의존성**: LSTM으로 감정+음성 변화의 시간적 패턴 학습
- **상호 보완**: 시각적 정보와 청각적 정보가 서로 보완하여 더 정확한 예측

## 기대 효과

- 유튜브 편집자들의 하이라이트 구간 찾기 자동화
- 스트리밍 플랫폼의 재밌는 순간 자동 클립 생성
- 콘텐츠 제작자의 편집 효율성 향상
- **더 정확한 재미도 예측** (시각+청각 멀티모달 활용)

> [!NOTE]
> 
> 1Dconv 로 처리하는건 어때? lstm 과 비교해서?

## Conv1D vs LSTM 비교

### Conv1D의 장점 ✅

- **병렬 처리**: GPU에서 훨씬 빠른 학습
- **Local pattern 감지**: "놀람→웃음" 같은 짧은 패턴 포착에 효과적
- **계산 효율성**: LSTM보다 가볍고 빠름
- **Gradient 안정**: Vanishing gradient 문제 없음

### LSTM의 장점 ✅

- **Long-term dependency**: 긴 시퀀스의 의존성 학습
- **순차적 특성**: 시간 순서가 중요한 데이터에 자연스러움
- **메모리 셀**: 이전 정보를 선택적으로 기억/망각

## 우리 프로젝트에서는?

### **Conv1D를 추천!** ⭐

#### 이유:

1. **짧은 시퀀스** (5-7초, 10-14프레임)
    - Conv1D가 충분히 전체 패턴 포착 가능
2. **Local pattern 중심**
    - "조용함→폭발적웃음" 같은 2-3프레임 패턴이 핵심
3. **12차원 입력**
    - 감정+오디오가 함께 변하는 패턴을 kernel이 효과적으로 학습
4. **학습 속도**
    - 데이터 수집/실험 단계에서 빠른 iteration 가능

## 구현 예시

```python
# Conv1D 구조
Conv1D(filters=64, kernel_size=3) → ReLU
Conv1D(filters=32, kernel_size=3) → ReLU  
GlobalMaxPooling1D()
Dense(1, sigmoid) → 재미도 예측

# 감정+음성 패턴을 3프레임 윈도우로 스캔
```

## 하이브리드 접근도 고려

나중에 성능이 부족하면:

```python
Conv1D → LSTM → Dense
# Conv1D로 local feature 추출 후 LSTM으로 시간 흐름 학습
```

**결론**: Conv1D부터 시작해서, 필요하면 LSTM 추가하는 방식이 효율적일 것 같습니다!